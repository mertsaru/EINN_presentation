\documentclass[
	aspectratio=169,	% Modern aspect ratio (TODO: Other ratios not yet supported)
	onlytextwidth,		% Sets totalwidth=\textwidth and therefore e.g. columns won't invade the margins
	t,					% Default vertical alignment of frames and colums at top (default is centered) % Stored in \beamer@centered (\beamer@centeredfalse, \beamer@centeredtrue)
%	handout,			% Create a basic handout of the presentation (removes overlays)
	]{beamer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1) Load the desired presentation theme
\usetheme[
% Individual options to customize the presentation conforming to the corporate design
	hs,					% Change default faculty color set and predefined faculty values: hs or <empty> (default), inw, cb, me, sw, wi, inst (\renewcommand{\insertfacultyname}{Institutname} needed)
	language=english,	% Change language to english (default: ngerman), other languages are possible (see babel-package) but may need further adjustments 
	toc,				% Adds a ToC slide
	sectionslide,		% Display separate section slides
	subsectionslide,		% Display separate subsection slides containing section and subsection name
%	smallpagenumber,	% Reduces the size of the page number
%	nototalpages,		% Hides the total number of slide in footline
%	nofacultyicon,		% Hides the faculty icon on title page
	colormath=nottext,	% Enables coloring of math text: off or <empty>, full, nottext (default)
%	printhandout,		% Places two slides on a single a4 paper for printing the presentation (beamer class option 'handout' needed)
%	noframesubtitle,	% Disables frame subtitles alltogether and slightly increases frame height
% Additional style options not completely conforming to the corporate design
	titlepagedate,		% Shows the date on the titelpage
	fancystyle,			% Enables some fancy styles, that are not part of the corporate design specifications (default: off)
%	progressbar,		% Shows the progressbar in footline (run twice to update progressbar)
	]{hsmw} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2) Specify default fields for presentation and pdf document properties
% Set the title: \title{Long title everywhere} or \title[Short title for footline]{Long title for titlepage}
\title[EINN: Epidemic Prediction using ML]{EINN: Epidemic Prediction using ML}
\subtitle{}
% Authors (separate multiple author names e.g. with \and for additional space): \author{author for everywhere} or \author[author for footline]{author for titlepage and thankyouslide}
\author[Mert Saruhan]{Mert Saruhan, B.Sc.}
% Institute (will be prefilled automatically, depending on chosen faculty theme option): \institute{institute for everywhere} or \institute[institute for footline and thankyouslide]{institute for titlepage}
%\institute[Fakultät Angewandte Computer und Biowissenschaften]{Fakultät Angewandte Computer und Biowissenschaften}
% Date of presentation (\today or a fixed value): \date{date for everywhere} or \date[date for footline]{date for titlepage}
\date{\today} % 22. März 2022
% Impressum for thankyou slide (leave one empty if not wanted or needed):
\email{msaruhan@hs-mittweida.de} % \email{E-Mail}
\courseofstudies{Mathematics for Network and Data Science (MA20w1-M)} % \courseofstudies{Course of Studies (Group Number)}
%\additional[Sidebar Text]{Main Text} % Additional information you may want to give (Department, Module, etc.) \additional[Sidebar Text]{Main Text}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3) Use features
% Titlegraphic changes the title page to "wide" (default) if left empty or inserts given image (by file path) and scales to 6.2cm height otherwise:
\titlegraphic{} %\titlegraphic{figures/thankyou.jpg}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4) Add bibliography
% Load the package and options you like, e.g. for recommended ieee style in combination of biblatex and biber:
\usepackage[backend=biber, bibstyle=ieee, citestyle=numeric-comp, sorting=none, natbib=true, hyperref=true, dashed=false]{biblatex}
% Add your bibliography file(s)
\addbibresource{ref.bib} 
% Dont forget to use \makebibliography where you want to put it, e.g. at the end of your presentation, after the "normal" slides:
% \appendix
% \makebibliography
% Compile with following command sequence to fully include bibliography: pdflatex, biber, pdflatex, pdflatex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5) Existing macro usage examples

% \appendix is used as end marker for slides (slide numbers and progressbar)
% \makethankyou creates a thank you slide and is used as end marker for slides (progressbar)
% \makebibliography creates one or multiple bibliography slides

% For multiple speakers you can use \setcurrentspeaker{speaker name} or \setcurrentspeaker*{speaker name} prior to the frame
% To reset to the default author, use \resetcurrentspeaker{} or \resetcurrentspeaker*{}
% Starred versions of these macros prepend the word "speaker"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 6) Import additional packages you need, (re-)define macros and create a wonderful latex presentation
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\setbeamertemplate{caption}{\insertcaption}
\usepackage{xcolor}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\section{What is EINN?}

\begin{frame}[fragile]{What is EINN?}{Introduction}

	\begin{itemize}
		\item<1-> EINN is a (\textbf{Neural Network}) machine learning model to predict epidemic dynamics~\cite{main}
		\item <2-> Mechanistic models (ODE models e.g SIR/SEIRM) good at trend approximation~\cite{main}
		\item <3-> RNN powered epidemic models good at short and long term epidemic prediction~\cite{main}
		\item<4-> EINN combines the knowledge of PINN, RNN, and ODE~\cite{main}
	\end{itemize}

\end{frame}

\section{Background}

\begin{frame}[fragile]{Background}{Mechanistic epidemic models}
	\begin{itemize}
		\item<1-> Uses ODE to predict
		\item<2-> Have many models: SIR, SEIR, SEIRM etc.
		\item<3-> Susceptible (S): $\frac{dS_{t}}{dt} = - \beta_{t} \frac{S_{t}I_{t}}{N}$
		\item<4-> Exposed (E): $\frac{dE_{t}}{dt} = \beta_{t} \frac{S_{t}I_{t}}{N} - \alpha_{t}E_{t}$
		\item<5-> Infected (I): $\frac{dI_{t}}{dt} = \alpha_{t}E_{t} - \gamma_{t}I_{t} - \mu_{t}I_{t}$
		\item<6-> Recovered (R): $ \frac{dR_{t}}{dt} = \gamma_{t} I_{t}$
		\item<7-> Mortality (M): $\frac{dM_{t}}{dt} = \mu_{t}I_{t}$
	\end{itemize}
	\vfill
	Note: $\Omega_{t} =\{\alpha_{t}, \beta_{t}, \gamma_{t}, \mu_{t} \}$ are parameters, $N$ is the population number, and $t$ is time.

\end{frame}

\begin{frame}[fragile]{Background}{RNN}
	\begin{columns}
		\begin{column}[T]{.33\textwidth}

			\begin{itemize}
				\item<1-> RNN is a neural network model
				\item<2-> Uses the previous state as an input to predict the next state
				\item<3-> Used in object detection and NLP
			\end{itemize}

		\end{column}

		\begin{column}[T]{.66\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/rnn-vs-fnn.png}
				\caption{Example structure of RNN and NN~\cite{rnnpic}}
			\end{figure}

		\end{column}

	\end{columns}
\end{frame}

\begin{frame}[fragile]{Background}{PINN}

	\begin{itemize}

		\item<1-> Uses physics laws to predict the next state
		\item<2-> Uses PDE (partial differential equation) and ODE (ordinary differential equation) as a loss estimator
		\item<3-> $\text{L}_{\text{data}}$: loss from data fitting
		\item<4-> $\text{L}_{\text{physics}}$: loss from physics laws \newline
			e.g. $\vec{a}= -\mu\Vert\vec{v}\Vert\vec{v} - \vec{g}$, $\vec{v} = \frac{df}{dt}$, $\vec{a}= \frac{d^{2}f}{dt^{2}}$ \newline \vfill
			$\text{L}_{\text{physics}} = \frac{1}{N}\sum_{i=0}^{N-1} \left(-\mu\Vert\frac{df_{i}}{dt_{i}}\Vert\frac{df_{i}}{dt_{i}} - \vec{g} - \left(\frac{d^{2}f_{i}}{dt_{i}^{2}}\right)\right)^{2}$
		\item<5-> $\text{L}_{\text{total}} = \text{L}_{\text{data}} + \text{L}_{\text{physics}}$

	\end{itemize}
	\vfill
	Note: $\vec{a}$ is acceleration, $\vec{v}$ is velocity, $\vec{g}$ is gravity, $\mu$ is friction, $f$ is position, $t$ is time, and $N$ is the number of data points.

\end{frame}

\begin{frame}[fragile]{Background}{PINN for Systems Biology}

	\begin{itemize}

		\item<1-> Recent works with PINN for Systems Biology~\cite{yazdani, karnia}: better PINN for SIR mechanics 
		\item<2-> Uses time $t$ as input for Neural Network $N (t)$ and rate of change in ODE systems $f_{\text{ODE}} (t)$
		\item<3-> Uses $ (\frac{dN (t)}{dt}-f_{\text{ODE}} (t))$ as a loss
		
	\end{itemize}

\end{frame}

\section{EINN Model}

\begin{frame}[fragile]{EINN Model}{Model summary}

	\begin{figure}
		\includegraphics[width=0.8\textwidth]{myfigs/scheme.png}
		\caption{EINN Model. (a) The training model. (b) Needed gradients to train the model using autograd. (c) ODE losses of feature module and time module. Taken from:~\cite{main}}

	\end{figure}

\end{frame}

\begin{frame}[fragile]{EINN Model}{Source model (Time module)}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemasource.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> Source model is a PINN for Systems Biology
				\item<2-> Input: time
				\item<3-> Output: embedding of the epidemic ODE states ($e_{t}$)
				% \item<5-> Uses autograd to compute loss
				\item<5-> Model freezes when $e_{t} \approx e_{t}^{F}$
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Time module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemasource2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item ODE loss for time module ($L^{\text{ODE}-T}$)~\cite{main}: $\frac{1}{N+1}\sum_{t=t_{0}}^{t_{N}} \left[ \frac{ds_{t}}{dt} - f_{\text{ODE}}(s_{t}, \Omega_{t}) \right] ^{2} $\newline
				$s_{t}$: predicted ODE states at time t using time module path
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Time module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemasource2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1->In the ODE states, Mortality (M) is observed and other states are latent~\cite{wu} 
				\item<2-> $L^{\text{Data-T}}$~\cite{main}: $\frac{1}{N+1}\sum_{t=t_{0}}^{t_{N}} \left[ \hat{M}_{t} - M_{t} \right]^{2}$ \newline
				M: mortality, $\hat{M}_{t}$: predicted mortality using time module path
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Time module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemasource2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> Adding monoticity loss (loss from latent states) to make learning less challenging using domain knowledge 
				\item<2->	$L^{\text{Mono}} = \frac{1}{N+1} (\sum_{t=t_{0}}^{t_{N}} \frac{dS_{t}}{dt}\text{ReLU} (\frac{dS_{t}}{dt}) + \sum_{t=t_{0}}^{t_{N}} -1\frac{dR_{t}}{dt}\text{ReLU} (-\frac{dR_{t}}{dt}))$
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Time module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemasource2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> Adding parameter to loss to track parameter change with time
				\item<2-> $L^{\text{Param}} = \frac{1}{N+1} \sum_{t=t_{0}}^{t_{N}} \left[ \Omega_{t+1} - \Omega_{t} \right]^{2} $
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}



%TODO what is time varying and mechanistic parameters
%TODO low frequency signals in NN, wang et al 2021b

\begin{frame}[fragile]{EINN Model}{Target model (Feature module)}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemafeature.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> Target model is an RNN model
				\item<2-> Input ($x_{t}$): Features relevant to the problem 
				\item<3-> Output: embedding of the epidemic ODE states ($e_{t}^{F}$)
				\item <4-> Model freezes when $e_{t} \approx e_{t}^{F}$
				\item <5-> Loss ($L^{\text{Emb}}$): $\frac{1}{N+1} \sum_{t=t_{0}}^{t_{N}} \left[ e_{t} - e_{t}^{F} \right]^{2} $
			\end{itemize}
		\end{column}
			
	\end{columns}
\end{frame}


\begin{frame}[fragile]{EINN Model}{Target module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemafeature2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> $e_{t}\approx e_{t}^{F}$ required for gradient trick for ODE loss from feature module
				\item<2-> $e_{t} \approx e_{t}^{F} \Rightarrow \frac{ds_{t}^{F}}{dt} = \frac{ds_{t}^{F}}{de_{t}^{F}} \frac{de_{t}^{F}}{dt} \approx \frac{ds_{t}^{F}}{de_{t}^{F}} \frac{de_{t}}{dt} $ 
				\item<3-> Loss ODE from feature module ($L^{\text{ODE}-F}$)~\cite{main}: $\frac{1}{N+1}\sum_{t=t_{0}}^{t_{N}} \left[ \frac{ds_{t}^{F}}{de_{t}^{F}}\frac{de_{t}}{dt} - f_{\text{ODE}}(s_{t}^{F}, \Omega_{t}) \right] ^{2} $
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Target module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemafeature2.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item<1-> $\hat{M}_{t}^{F}$: predicted mortality using feature module path
				\item<2-> Loss from data ($L^{\text{Data}-F}$): $\frac{1}{N+1}\sum_{t=t_{0}}^{t_{N}}\left[ \hat{M}_{t}^{F} - M_{t}\right]^{2}$   
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}

\begin{frame}[fragile]{EINN Model}{Target module losses}
	\begin{columns}
		
		\begin{column}[T]{.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{myfigs/schemaoutput.png}
				\caption{Cropped model summary showing the training summary. Cropped from:~\cite{main}}
			\end{figure}
		\end{column}
		
		\begin{column}[T]{.5\textwidth}
			\begin{itemize}
				\item <1-> N$_{\text{output}}$ feed-forward Neural Network
				\item<2-> Alligning findings from both paths
				\item<3-> $L^{\text{Output}} = \frac{1}{N+1}\sum_{t=t_{0}}^{t_{N}} \left[ s_{t} - s_{t}^{F} \right]^{2}$
			\end{itemize}
		\end{column}
			
\end{columns}
\end{frame}


\section{Results}

\begin{frame}[fragile]{Results}

	\begin{itemize}

		\item Generation: The NN learns directly from ODE generated data~\cite{main, wang, sanchez}
		\item Regularization: The NN predicts states and parameters of an ODE and learns from the parameters~\cite{main, gao, gaw}
		\item Ensembling: Combines predictions of RNN and SEIRM/SIR outputs in one NN for a final prediction~\cite{main, adiga, yamana}
		\item EINNs-NoGradMatching: skipping $e_{t} \approx e_{t}^{F}$ part~\cite{main}

	\end{itemize}

\end{frame}

\begin{frame}[fragile]{Results}

	\begin{figure}
		\includegraphics[width=0.5\textwidth]{myfigs/result.png}
		\caption{Short-term, Long-term, and Trend results. NR: Normalized RMSE (Lower is better), ND: Normal Deviation (Lower is better) PC: Pearson Correlation (Higher is better). Image taken from:~\cite{main}}
	\end{figure}

\end{frame}

\begin{frame}[fragile]{Results}

\begin{itemize}
	\item Better short and long term prediction than RNN and Mechanistic model (SEIRM)
	\item Better trend correlation than RNN and sligtly less trend correlation than Mechanistic model (SEIRM)
\end{itemize}
\end{frame}


\appendix

\makebibliography

\makethankyou

\end{document}
